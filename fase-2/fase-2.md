## ðŸŽ® Fase 2: VisÃ£o Geral do Projeto

### O que vamos construir?

Uma IA que aprende a jogar **Jogo da Velha** (Tic-Tac-Toe) do zero usando **Q-Learning**. A IA vai:

1. ComeÃ§ar sem saber nada (jogando aleatoriamente)
2. Jogar **milhares de partidas** contra ela mesma (Self-Play)
3. Aprender com cada vitÃ³ria, derrota e empate
4. ApÃ³s o treino, **nunca mais perder** uma partida!

### Por que Jogo da Velha?

Ã‰ perfeito para aprender RL porque:

* **Simples:** Tabuleiro 3x3, apenas 2 jogadores
* **Finito:** NÃºmero limitado de estados possÃ­veis (~5.478 estados Ãºnicos)
* **RÃ¡pido de treinar:** Leva segundos, nÃ£o horas
* **Demonstra tudo:** VocÃª verÃ¡ o aprendizado acontecendo visualmente

---

## ðŸ“š O que Ã© Q-Learning?

Agora vou explicar o algoritmo que usaremos. Preste bastante atenÃ§Ã£o!

### A Ideia Central

**Q-Learning** Ã© um dos algoritmos de RL mais famosos e simples. O nome "Q" vem de "**Quality**" (qualidade).

O algoritmo cria uma **Q-Table** (Tabela Q) que funciona como uma "**folha de cola**" gigante para o agente:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Q-TABLE = "Quanto vale fazer X em Y?"         â”‚
â”‚                                                 â”‚
â”‚  Estado â†’ | AÃ§Ã£o 1 | AÃ§Ã£o 2 | AÃ§Ã£o 3 | ...     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  Estado A |  0.5   |  0.2   |  0.9   | ...     â”‚
â”‚  Estado B | -0.3   |  0.8   |  0.1   | ...     â”‚
â”‚  Estado C |  0.7   |  0.4   | -0.2   | ...     â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Cada cÃ©lula guarda um **valor Q**: quanto aquela aÃ§Ã£o vale naquele estado.

### Analogia: Mapa do Tesouro

Imagine que vocÃª estÃ¡ em uma ilha procurando tesouros:

* Cada local da ilha = um **Estado**
* Cada direÃ§Ã£o (norte, sul...) = uma **AÃ§Ã£o**
* A Q-Table Ã© seu **mapa** com anotaÃ§Ãµes que vÃ£o melhorando com o tempo

---

## ðŸ”¢ A EquaÃ§Ã£o de Bellman

Q-Learning usa uma fÃ³rmula matemÃ¡tica chamada **EquaÃ§Ã£o de Bellman** para atualizar a Q-Table:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \times [r + \gamma \times \max Q(s', a') - Q(s, a)]
$$

### TraduÃ§Ã£o dos sÃ­mbolos

| SÃ­mbolo | Significa                   |
| ------- | --------------------------- |
| Q(s, a) | Valor da aÃ§Ã£o a no estado s |
| s       | Estado atual                |
| a       | AÃ§Ã£o tomada                 |
| r       | Recompensa                  |
| s'      | PrÃ³ximo estado              |
| Î±       | Taxa de aprendizado         |
| Î³       | Fator de desconto           |

### Em PortuguÃªs Claro

> Ajuste o valor Q baseado na diferenÃ§a entre o que vocÃª **esperava** e o que realmente **aconteceu**.

---

## ðŸŽ¯ Q-Learning no Jogo da Velha

### Estados (s)

Cada configuraÃ§Ã£o possÃ­vel do tabuleiro:

```
X | O | X
---------
  | X | O
---------
  |   |  
```

Representaremos como uma tupla:
`('X','O','X','','X','O','','','')`

### AÃ§Ãµes (a)

Escolher uma posiÃ§Ã£o vazia:

```
0 | 1 | 2
---------
3 | 4 | 5
---------
6 | 7 | 8
```

### Recompensas (r)

| SituaÃ§Ã£o          | Recompensa |
| ----------------- | ---------- |
| VitÃ³ria           | +1         |
| Derrota           | -1         |
| Empate            | 0          |
| Jogo em andamento | 0          |

---

## ðŸ“‹ Estrutura do Projeto - Fase 2

```
ai-game-learning/
â”œâ”€â”€ fase2_jogo_velha/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ambiente.py
â”‚   â”œâ”€â”€ agente.py
â”‚   â”œâ”€â”€ treinador.py
â”‚   â”œâ”€â”€ jogar.py
â”‚   â””â”€â”€ visualizador.py
```

* **ambiente.py**: lÃ³gica do jogo
* **agente.py**: Q-Learning
* **treinador.py**: self-play
* **jogar.py**: jogar contra a IA treinada
* **visualizador.py**: grÃ¡ficos

---

## ðŸš€ O Processo de Treinamento

### Exemplo do comeÃ§o do treino:

* Muitos movimentos aleatÃ³rios
* IA ganhando ou perdendo por sorte
* Q-Table comeÃ§a com tudo **0**

### Final do treino:

* Jogadas sempre Ã³timas
* A IA fica praticamente **imbatÃ­vel**

---

## ðŸŽ“ Conceitos Novos: Epsilon-Greedy

Usamos o parÃ¢metro **Îµ (epsilon)** para equilibrar:

| EstratÃ©gia         | O que faz               |
| ------------------ | ----------------------- |
| Explorar           | Tentar movimentos novos |
| Explorar (Exploit) | Usar a Q-Table          |

PseudocÃ³digo:

```python
if random() < epsilon:
    aÃ§Ã£o = escolher_aleatoriamente()
else:
    aÃ§Ã£o = aÃ§Ã£o_com_maior_Q()
```

* No inÃ­cio: **epsilon alto â†’ muita exploraÃ§Ã£o**
* No final: **epsilon baixo â†’ comportamento Ã³timo**

---

## ðŸ“ Resumo da Fase 2

VocÃª vai aprender:

âœ… Q-Learning do zero
âœ… Criar um ambiente de jogo
âœ… Treinar via Self-Play
âœ… Epsilon-Greedy
âœ… Visualizar resultados

### HiperparÃ¢metros que usaremos:

| ParÃ¢metro               | Valor  |
| ----------------------- | ------ |
| Î± (Taxa de aprendizado) | 0.5    |
| Î³ (Desconto)            | 0.9    |
| EpisÃ³dios               | 20.000 |
| Îµ inicial               | 0.8    |
| Îµ final                 | 0.1    |

---