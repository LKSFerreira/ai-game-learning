"""
M√≥dulo: üß† agente.py
Projeto: üìò AI Game Learning

Este m√≥dulo define o Agente que utiliza o algoritmo Q-Learning.
No paradigma de Aprendizado por Refor√ßo, este c√≥digo representa o "Agent".

Responsabilidades do Agente:
- Manter uma "mem√≥ria de jogo", a Tabela Q (Q-Table).
- Decidir qual a√ß√£o tomar, balanceando entre explorar e usar seu conhecimento.
- Aprender com os resultados de suas a√ß√µes, atualizando sua mem√≥ria.
"""

import random
import pickle
from typing import List, Tuple, Dict
from pathlib import Path

class AgenteQLearning:
    """
    Um Agente que aprende a jogar Jogo da Velha usando Q-Learning.
    
    Pense neste Agente como um jogador de Ragnarok Online que est√° aprendendo
    a melhor estrat√©gia para derrotar monstros.

    Hiperpar√¢metros (os "atributos" do nosso jogador):
    - alpha (Œ±): A "Velocidade de Aprendizado".
      * Qu√£o r√°pido o jogador ajusta sua estrat√©gia ap√≥s uma batalha.
      * Valores altos = impulsivo, aprende r√°pido com uma √∫nica experi√™ncia.
      * Valores baixos = c√©tico, precisa de muitas experi√™ncias para mudar de ideia.

    - gamma (Œ≥): A "Vis√£o de Futuro" (Fator de Desconto).
      * O quanto o jogador valoriza recompensas futuras.
      * Valor alto = estrategista, pensa nos pr√≥ximos passos.
      * Valor baixo = imediatista, foca apenas na recompensa de agora.

    - epsilon (Œµ): O "Medidor de Curiosidade" (Taxa de Explora√ß√£o).
      * A chance do jogador tentar uma t√°tica nova e desconhecida.
      * Valor alto = aventureiro, adora explorar o mapa.
      * Valor baixo = conservador, prefere usar a t√°tica que j√° sabe que funciona.
    """

    def __init__(self,
                 alpha: float = 0.5,
                 gamma: float = 0.9,
                 epsilon: float = 1.0,
                 epsilon_minimo: float = 0.01,
                 taxa_decaimento_epsilon: float = 0.9995,
                 jogador: int = 1):
        """
        Inicializa os atributos e a mem√≥ria do Agente.
        """
        # --- HIPERPAR√ÇMETROS (Atributos do Agente) ---
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_minimo = epsilon_minimo
        self.taxa_decaimento_epsilon = taxa_decaimento_epsilon

        # --- IDENTIDADE ---
        self.jogador = jogador
        self.simbolo = 'X' if jogador == 1 else 'O'

        # --- MEM√ìRIA (A "Enciclop√©dia de Monstros" do Jogador) ---
        # A Tabela Q armazena o valor de cada "t√°tica" (a√ß√£o) em cada
        # "situa√ß√£o de batalha" (estado).
        # Estrutura: { estado_do_tabuleiro: { acao: valor_q } }
        self.tabela_q: Dict[Tuple, Dict[int, float]] = {}

        # --- ESTAT√çSTICAS DE TREINO ---
        self.partidas_treinadas = 0
        self.vitorias = 0
        self.derrotas = 0
        self.empates = 0

    def obter_valor_q(self, estado: Tuple, acao: int) -> float:
        """
        Consulta a "mem√≥ria" para ver o valor de uma a√ß√£o em um estado.
        Se o Agente nunca viu essa situa√ß√£o antes, ele assume que o valor √© 0.
        
        Args:
            estado: A configura√ß√£o atual do tabuleiro.
            acao: A jogada que queremos consultar.
            
        Returns:
            O valor Q aprendido para o par (estado, a√ß√£o).
        """
        # Se o estado √© novo, adiciona uma nova p√°gina √† "enciclop√©dia".
        if estado not in self.tabela_q:
            self.tabela_q[estado] = {}
        
        # Se a a√ß√£o nunca foi tentada nesse estado, anota com valor inicial 0.
        if acao not in self.tabela_q[estado]:
            self.tabela_q[estado][acao] = 0.0
            
        return self.tabela_q[estado][acao]

    def atualizar_valor_q(self, estado: Tuple, acao: int, recompensa: float, proximo_estado: Tuple):
        """
        Atualiza a "mem√≥ria" do Agente usando a Equa√ß√£o de Bellman.
        √â aqui que o aprendizado realmente acontece.
        
        F√≥rmula em "linguagem gamer":
        NovaOpini√£o = Opini√£oAntiga + VelocidadeAprendizado * (RecompensaReal - Opini√£oAntiga)
        
        Onde a RecompensaReal = (O que ganhei agora + Potencial da pr√≥xima jogada)
        """
        # 1. Pega a opini√£o antiga (o valor Q que o agente *achava* que a jogada valia).
        opiniao_antiga = self.obter_valor_q(estado, acao)

        # 2. Calcula o melhor resultado poss√≠vel a partir do pr√≥ximo estado.
        #    √â o "potencial da pr√≥xima jogada".
        melhor_valor_futuro = self._obter_melhor_valor_q_do_estado(proximo_estado)

        # 3. Calcula o valor que a jogada *realmente* teve.
        valor_real_da_jogada = recompensa + self.gamma * melhor_valor_futuro

        # 4. A "surpresa" ou "erro de previs√£o" √© a diferen√ßa entre o real e o esperado.
        surpresa = valor_real_da_jogada - opiniao_antiga

        # 5. Atualiza a opini√£o antiga, ajustando-a um pouco na dire√ß√£o da surpresa.
        #    O `alpha` controla o "tamanho do passo" desse ajuste.
        novo_valor_q = opiniao_antiga + self.alpha * surpresa
        
        self.tabela_q[estado][acao] = novo_valor_q

    def _obter_melhor_valor_q_do_estado(self, estado: Tuple) -> float:
        """
        Verifica na "mem√≥ria" qual √© a melhor jogada poss√≠vel a partir de um estado.
        
        Returns:
            O maior valor Q para o estado fornecido. Retorna 0 se o estado for novo.
        """
        if estado not in self.tabela_q or not self.tabela_q[estado]:
            return 0.0
        return max(self.tabela_q[estado].values())

    def escolher_acao(self, estado: Tuple, acoes_validas: List[int], em_treinamento: bool = True) -> int:
        """
        Decide qual jogada fazer usando a estrat√©gia Epsilon-Greedy.
        
        Args:
            estado: A configura√ß√£o atual do tabuleiro.
            acoes_validas: Lista de jogadas permitidas.
            em_treinamento: Se True, usa o "Medidor de Curiosidade" (epsilon).
                            Se False, sempre usa a melhor t√°tica conhecida.
        
        Returns:
            A a√ß√£o (√≠ndice da casa) escolhida pelo Agente.
        """
        if not acoes_validas:
            raise ValueError("N√£o h√° a√ß√µes v√°lidas para escolher.")

        # Se n√£o estiver em treinamento, joga para ganhar (sempre a melhor t√°tica).
        if not em_treinamento:
            return self._escolher_melhor_acao(estado, acoes_validas)

        # L√≥gica Epsilon-Greedy:
        if random.random() < self.epsilon:
            # "Modo Aventura": Tenta uma t√°tica aleat√≥ria para explorar.
            return random.choice(acoes_validas)
        else:
            # "Modo Farm": Usa a melhor t√°tica conhecida para garantir o resultado.
            return self._escolher_melhor_acao(estado, acoes_validas)

    def _escolher_melhor_acao(self, estado: Tuple, acoes_validas: List[int]) -> int:
        """
        Consulta a "mem√≥ria" e escolhe a a√ß√£o com o maior valor Q.
        Se houver empate entre as melhores a√ß√µes, escolhe uma delas aleatoriamente.
        """
        valores_q_das_acoes = {acao: self.obter_valor_q(estado, acao) for acao in acoes_validas}
        
        valor_maximo_q = max(valores_q_das_acoes.values())
        
        melhores_acoes = [acao for acao, valor in valores_q_das_acoes.items() if valor == valor_maximo_q]
        
        return random.choice(melhores_acoes)

    def aprender_com_partida(self, historico_da_partida: List, recompensa_final: float):
        """
        Processa o hist√≥rico de uma partida finalizada para aprender com ela.
        Este m√©todo √© chamado pelo Treinador ao final de cada jogo.
        
        Pense nisso como o jogador, ap√≥s derrotar um MVP, refletindo sobre
        todas as a√ß√µes que o levaram √† vit√≥ria.
        """
        self.partidas_treinadas += 1
        if recompensa_final > 0: self.vitorias += 1
        elif recompensa_final < 0: self.derrotas += 1
        else: self.empates += 1

        # Propaga a recompensa final para tr√°s, valorizando as jogadas
        # que levaram a este resultado.
        for estado, acao, proximo_estado in reversed(historico_da_partida):
            self.atualizar_valor_q(estado, acao, recompensa_final, proximo_estado)
            # A recompensa perde um pouco de for√ßa a cada passo para tr√°s,
            # controlado pela "Vis√£o de Futuro" (gamma).
            recompensa_final *= self.gamma
        
        self.reduzir_epsilon()

    def reduzir_epsilon(self):
        """
        Reduz a "curiosidade" do Agente, tornando-o mais confiante em seu
        conhecimento com o passar do tempo.
        """
        self.epsilon = max(self.epsilon_minimo, self.epsilon * self.taxa_decaimento_epsilon)

    def salvar_memoria(self, caminho: str = "agente_treinado.pkl"):
        """
        Salva o conhecimento do Agente (a Tabela Q e os hiperpar√¢metros) em um arquivo.
        """
        caminho_arquivo = Path(caminho)
        caminho_arquivo.parent.mkdir(parents=True, exist_ok=True)
        
        with open(caminho_arquivo, 'wb') as arquivo:
            pickle.dump(self.tabela_q, arquivo)
        print(f"üíæ Mem√≥ria do Agente salva em: {caminho_arquivo}")

    def carregar_memoria(self, caminho: str):
        """
        Carrega o conhecimento de um Agente previamente treinado.
        """
        caminho_arquivo = Path(caminho)
        if not caminho_arquivo.exists():
            print(f"‚ö†Ô∏è  Aviso: Nenhum arquivo de mem√≥ria encontrado em {caminho}. O Agente come√ßar√° do zero.")
            return

        with open(caminho_arquivo, 'rb') as arquivo:
            self.tabela_q = pickle.load(arquivo)
        print(f"‚úÖ Mem√≥ria do Agente carregada de: {caminho_arquivo}")
        print(f"   - O Agente conhece {len(self.tabela_q):,} situa√ß√µes de jogo.")