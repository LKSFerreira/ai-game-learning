"""
Agente Q-Learning para Jogo da Velha

Este m√≥dulo implementa o "c√©rebro" que aprende a jogar.
Representa o AGENT no Reinforcement Learning.

Responsabilidades:
- Manter a Q-Table (mem√≥ria de valores aprendidos)
- Escolher a√ß√µes (explora√ß√£o vs explora√ß√£o)
- Atualizar valores Q usando a Equa√ß√£o de Bellman
- Salvar e carregar o modelo treinado

Componentes principais:
- Q-Table: Dicion√°rio {estado: {a√ß√£o: valor_q}}
- Epsilon-Greedy: Estrat√©gia de explora√ß√£o
- Learning Rate (Œ±): Velocidade de aprendizado
- Discount Factor (Œ≥): Import√¢ncia de recompensas futuras
"""

import numpy as np
import random
import pickle
from typing import Tuple, List, Dict, Optional
from pathlib import Path


class AgenteQLearning:
    """
    Agente que aprende a jogar Jogo da Velha usando Q-Learning.
    
    O agente mant√©m uma Q-Table que mapeia:
    - Estado do jogo ‚Üí A√ß√µes poss√≠veis ‚Üí Valores Q
    
    Quanto maior o valor Q, melhor a a√ß√£o naquele estado.
    
    Par√¢metros de treinamento:
    - alpha (Œ±): Taxa de aprendizado (0.0 a 1.0)
      * 0.0 = n√£o aprende nada
      * 1.0 = substitui completamente conhecimento antigo
      * Recomendado: 0.3 a 0.7
    
    - gamma (Œ≥): Fator de desconto (0.0 a 1.0)
      * 0.0 = s√≥ importa recompensa imediata
      * 1.0 = recompensas futuras t√™m igual import√¢ncia
      * Recomendado: 0.8 a 0.99
    
    - epsilon (Œµ): Taxa de explora√ß√£o (0.0 a 1.0)
      * 0.0 = sempre explora√ß√£o (greedy)
      * 1.0 = sempre explora√ß√£o (aleat√≥rio)
      * Recomendado iniciar: 0.8, terminar: 0.1
    """
    
    def __init__(
        self,
        alpha: float = 0.5,
        gamma: float = 0.9,
        epsilon: float = 1.0,
        epsilon_min: float = 0.01,
        epsilon_decay: float = 0.9995,
        jogador: int = 1
    ):
        """
        Inicializa o agente Q-Learning.
        
        Args:
            alpha: Taxa de aprendizado (velocidade que aprende)
            gamma: Fator de desconto (valor de recompensas futuras)
            epsilon: Taxa inicial de explora√ß√£o (chance de a√ß√£o aleat√≥ria)
            epsilon_min: Taxa m√≠nima de explora√ß√£o (n√£o vai abaixo disso)
            epsilon_decay: Multiplicador de decay do epsilon (0.0 a 1.0)
            jogador: Qual jogador este agente representa (1=X ou 2=O)
        """
        # === HIPERPAR√ÇMETROS ===
        self.alpha = alpha  # Taxa de aprendizado (Œ±)
        self.gamma = gamma  # Fator de desconto (Œ≥)
        self.epsilon = epsilon  # Taxa de explora√ß√£o atual (Œµ)
        self.epsilon_min = epsilon_min  # Limite inferior do epsilon
        self.epsilon_decay = epsilon_decay  # Taxa de decaimento
        
        # === IDENTIDADE ===
        self.jogador = jogador  # 1 (X) ou 2 (O)
        self.simbolo = 'X' if jogador == 1 else 'O'
        
        # === MEM√ìRIA (Q-TABLE) ===
        # Estrutura: {estado: {a√ß√£o: valor_q}}
        # Exemplo: {(1,0,2,0,0,0,0,0,0): {3: 0.5, 4: 0.8, ...}}
        self.q_table: Dict[Tuple, Dict[int, float]] = {}
        
        # === ESTAT√çSTICAS ===
        self.episodios_treinados = 0
        self.vitorias = 0
        self.derrotas = 0
        self.empates = 0
        
        # === HIST√ìRICO DO EPIS√ìDIO ATUAL ===
        # Guarda (estado, a√ß√£o) para atualizar Q-values no final
        self.historico_episodio: List[Tuple[Tuple, int]] = []
    
    def obter_valor_q(self, estado: Tuple, acao: int) -> float:
        """
        Retorna o valor Q para um par (estado, a√ß√£o).
        
        Se o par nunca foi visto antes, inicializa com 0.0.
        
        Args:
            estado: Tupla representando o tabuleiro
            acao: √çndice da a√ß√£o (0-8)
        
        Returns:
            Valor Q atual para esse par (estado, a√ß√£o)
        
        Exemplo:
            estado = (1, 0, 0, 0, 0, 0, 0, 0, 0)
            acao = 4  # Jogar no centro
            valor = agente.obter_valor_q(estado, acao)  # Retorna 0.0 se novo
        """
        # Se o estado nunca foi visto, inicializa com dicion√°rio vazio
        if estado not in self.q_table:
            self.q_table[estado] = {}
        
        # Se a a√ß√£o nunca foi tentada nesse estado, inicializa com 0.0
        if acao not in self.q_table[estado]:
            self.q_table[estado][acao] = 0.0
        
        return self.q_table[estado][acao]
    
    def atualizar_valor_q(
        self,
        estado: Tuple,
        acao: int,
        recompensa: float,
        proximo_estado: Tuple,
        finalizado: bool
    ):
        """
        Atualiza o valor Q usando a Equa√ß√£o de Bellman.
        
        F√≥rmula:
        Q(s,a) ‚Üê Q(s,a) + Œ± √ó [r + Œ≥ √ó max(Q(s',a')) - Q(s,a)]
        
        Onde:
        - s = estado atual
        - a = a√ß√£o tomada
        - r = recompensa recebida
        - s' = pr√≥ximo estado
        - a' = melhor a√ß√£o no pr√≥ximo estado
        - Œ± = taxa de aprendizado
        - Œ≥ = fator de desconto
        
        Args:
            estado: Estado antes da a√ß√£o
            acao: A√ß√£o executada
            recompensa: Recompensa recebida
            proximo_estado: Estado ap√≥s a a√ß√£o
            finalizado: Se o jogo terminou
        """
        # Valor Q atual para (estado, a√ß√£o)
        q_atual = self.obter_valor_q(estado, acao)
        
        # Se o jogo terminou, n√£o h√° valor futuro
        if finalizado:
            q_futuro_max = 0.0
        else:
            # Encontra o maior valor Q poss√≠vel no pr√≥ximo estado
            # Isso representa "quanto vale estar no pr√≥ximo estado"
            q_futuro_max = self._obter_max_q_futuro(proximo_estado)
        
        # === EQUA√á√ÉO DE BELLMAN ===
        # Calcula o "alvo" (target): recompensa + valor futuro descontado
        target = recompensa + self.gamma * q_futuro_max
        
        # Atualiza o valor Q fazendo um "passo" em dire√ß√£o ao alvo
        # Alpha controla o tamanho do passo (quanto aprender)
        novo_q = q_atual + self.alpha * (target - q_atual)
        
        # Salva o novo valor na Q-Table
        if estado not in self.q_table:
            self.q_table[estado] = {}
        self.q_table[estado][acao] = novo_q
    
    def _obter_max_q_futuro(self, estado: Tuple) -> float:
        """
        Retorna o maior valor Q dispon√≠vel em um estado.
        
        Isso representa "qu√£o boa √© a melhor a√ß√£o poss√≠vel neste estado".
        
        Args:
            estado: Estado para analisar
        
        Returns:
            O maior valor Q entre todas as a√ß√µes nesse estado
            Retorna 0.0 se o estado nunca foi visto
        """
        # Se o estado √© novo, n√£o temos informa√ß√£o
        if estado not in self.q_table:
            return 0.0
        
        # Se n√£o h√° a√ß√µes registradas nesse estado
        if not self.q_table[estado]:
            return 0.0
        
        # Retorna o maior valor Q entre todas as a√ß√µes
        return max(self.q_table[estado].values())
    
    def escolher_acao(
        self,
        estado: Tuple,
        acoes_validas: List[int],
        treino: bool = True
    ) -> int:
        """
        Escolhe uma a√ß√£o usando a estrat√©gia Epsilon-Greedy.
        
        Com probabilidade Œµ: EXPLORA√á√ÉO (a√ß√£o aleat√≥ria)
        Com probabilidade 1-Œµ: EXPLORA√á√ÉO (melhor a√ß√£o conhecida)
        
        Args:
            estado: Estado atual do jogo
            acoes_validas: Lista de a√ß√µes poss√≠veis (casas vazias)
            treino: Se True, usa epsilon. Se False, sempre greedy.
        
        Returns:
            √çndice da a√ß√£o escolhida (0-8)
        
        Exemplo:
            estado = (1, 0, 2, 0, 0, 0, 0, 0, 0)
            acoes = [1, 3, 4, 5, 6, 7, 8]
            acao = agente.escolher_acao(estado, acoes, treino=True)
        """
        # Valida√ß√£o: verifica se h√° a√ß√µes v√°lidas
        if not acoes_validas:
            raise ValueError("N√£o h√° a√ß√µes v√°lidas dispon√≠veis!")
        
        # === MODO AVALIA√á√ÉO (sem explora√ß√£o) ===
        if not treino:
            return self._escolher_melhor_acao(estado, acoes_validas)
        
        # === EPSILON-GREEDY ===
        # Gera n√∫mero aleat√≥rio entre 0 e 1
        if random.random() < self.epsilon:
            # EXPLORA√á√ÉO: escolhe a√ß√£o aleat√≥ria
            return random.choice(acoes_validas)
        else:
            # EXPLORA√á√ÉO: escolhe melhor a√ß√£o conhecida
            return self._escolher_melhor_acao(estado, acoes_validas)
    
    def _escolher_melhor_acao(
        self,
        estado: Tuple,
        acoes_validas: List[int]
    ) -> int:
        """
        Escolhe a a√ß√£o com maior valor Q (estrat√©gia greedy).
        
        Se m√∫ltiplas a√ß√µes t√™m o mesmo valor, escolhe aleatoriamente
        entre elas para evitar vi√©s.
        
        Args:
            estado: Estado atual
            acoes_validas: A√ß√µes poss√≠veis
        
        Returns:
            A√ß√£o com maior valor Q
        """
        # Obt√©m valores Q de todas as a√ß√µes v√°lidas
        valores_q = {
            acao: self.obter_valor_q(estado, acao)
            for acao in acoes_validas
        }
        
        # Encontra o maior valor Q
        max_q = max(valores_q.values())
        
        # Encontra todas as a√ß√µes que t√™m esse valor m√°ximo
        # (pode haver empate)
        melhores_acoes = [
            acao for acao, valor in valores_q.items()
            if valor == max_q
        ]
        
        # Se h√° empate, escolhe aleatoriamente entre as melhores
        return random.choice(melhores_acoes)
    
    def iniciar_episodio(self):
        """
        Prepara o agente para um novo epis√≥dio de treinamento.
        
        Limpa o hist√≥rico de (estado, a√ß√£o) do epis√≥dio anterior.
        """
        self.historico_episodio = []
    
    def registrar_jogada(self, estado: Tuple, acao: int):
        """
        Registra uma jogada no hist√≥rico do epis√≥dio atual.
        
        Isso √© usado para atualizar valores Q no final do jogo,
        especialmente quando o resultado s√≥ √© conhecido no fim.
        
        Args:
            estado: Estado antes da jogada
            acao: A√ß√£o executada
        """
        self.historico_episodio.append((estado, acao))
    
    def finalizar_episodio(self, recompensa_final: float):
        """
        Atualiza valores Q no final de um epis√≥dio.
        
        Propaga a recompensa final para todas as jogadas do epis√≥dio,
        com decaimento baseado em gamma.
        
        Args:
            recompensa_final: +1 (vit√≥ria), -1 (derrota), 0 (empate)
        """
        # Atualiza estat√≠sticas
        self.episodios_treinados += 1
        if recompensa_final > 0:
            self.vitorias += 1
        elif recompensa_final < 0:
            self.derrotas += 1
        else:
            self.empates += 1
        
        # Propaga recompensa para todas as jogadas do epis√≥dio
        # Come√ßa da jogada mais recente (mais importante)
        recompensa_propagada = recompensa_final
        
        for estado, acao in reversed(self.historico_episodio):
            # Atualiza o valor Q dessa jogada
            self.atualizar_valor_q(
                estado=estado,
                acao=acao,
                recompensa=recompensa_propagada,
                proximo_estado=estado,  # Placeholder
                finalizado=True
            )
            
            # Decai a recompensa para jogadas anteriores
            recompensa_propagada *= self.gamma
        
        # Reduz epsilon (menos explora√ß√£o com o tempo)
        self.decair_epsilon()
    
    def decair_epsilon(self):
        """
        Reduz o epsilon (explora√ß√£o) gradualmente.
        
        Usa decay exponencial: Œµ = max(Œµ_min, Œµ √ó decay_rate)
        
        Isso faz o agente explorar muito no in√≠cio e
        explorar (usar conhecimento) mais no final do treino.
        """
        self.epsilon = max(
            self.epsilon_min,
            self.epsilon * self.epsilon_decay
        )
    
    def salvar(self, caminho: str = "agente_treinado.pkl"):
        """
        Salva o agente treinado em um arquivo.
        
        Usa o m√≥dulo pickle do Python para serializa√ß√£o.
        
        Args:
            caminho: Nome do arquivo onde salvar
        
        Exemplo:
            agente.salvar("jogador_x_50k_episodios.pkl")
        """
        # Prepara dados para salvar
        dados = {
            'q_table': self.q_table,
            'alpha': self.alpha,
            'gamma': self.gamma,
            'epsilon': self.epsilon,
            'epsilon_min': self.epsilon_min,
            'epsilon_decay': self.epsilon_decay,
            'jogador': self.jogador,
            'episodios_treinados': self.episodios_treinados,
            'vitorias': self.vitorias,
            'derrotas': self.derrotas,
            'empates': self.empates
        }
        
        # Salva em arquivo bin√°rio
        caminho_arquivo = Path(caminho)
        caminho_arquivo.parent.mkdir(parents=True, exist_ok=True)
        
        with open(caminho_arquivo, 'wb') as arquivo:
            pickle.dump(dados, arquivo)
        
        # print(f"‚úÖ Agente salvo em: {caminho}")
        # print(f"   Epis√≥dios treinados: {self.episodios_treinados:,}")
        # print(f"   Tamanho da Q-Table: {len(self.q_table):,} estados")
    
    @classmethod
    def carregar(cls, caminho: str) -> 'AgenteQLearning':
        """
        Carrega um agente previamente treinado.
        
        Args:
            caminho: Caminho do arquivo .pkl
        
        Returns:
            Nova inst√¢ncia de AgenteQLearning com dados carregados
        
        Exemplo:
            agente = AgenteQLearning.carregar("jogador_x_50k.pkl")
        """
        with open(caminho, 'rb') as arquivo:
            dados = pickle.load(arquivo)
        
        # Cria novo agente com par√¢metros carregados
        agente = cls(
            alpha=dados['alpha'],
            gamma=dados['gamma'],
            epsilon=dados['epsilon'],
            epsilon_min=dados['epsilon_min'],
            epsilon_decay=dados['epsilon_decay'],
            jogador=dados['jogador']
        )
        
        # Restaura Q-Table e estat√≠sticas
        agente.q_table = dados['q_table']
        agente.episodios_treinados = dados['episodios_treinados']
        agente.vitorias = dados['vitorias']
        agente.derrotas = dados['derrotas']
        agente.empates = dados['empates']
        
        print(f"‚úÖ Agente carregado de: {caminho}")
        print(f"   Epis√≥dios treinados: {agente.episodios_treinados:,}")
        print(f"   Tamanho da Q-Table: {len(agente.q_table):,} estados")
        
        return agente
    
    def obter_estatisticas(self) -> Dict[str, any]:
        """
        Retorna estat√≠sticas de treinamento do agente.
        
        Returns:
            Dicion√°rio com m√©tricas de desempenho
        """
        total_jogos = self.vitorias + self.derrotas + self.empates
        
        if total_jogos == 0:
            return {
                'episodios': 0,
                'estados_conhecidos': len(self.q_table),
                'taxa_vitoria': 0.0,
                'taxa_empate': 0.0,
                'taxa_derrota': 0.0,
                'epsilon_atual': self.epsilon
            }
        
        return {
            'episodios': self.episodios_treinados,
            'estados_conhecidos': len(self.q_table),
            'vitorias': self.vitorias,
            'derrotas': self.derrotas,
            'empates': self.empates,
            'taxa_vitoria': self.vitorias / total_jogos,
            'taxa_empate': self.empates / total_jogos,
            'taxa_derrota': self.derrotas / total_jogos,
            'epsilon_atual': self.epsilon,
            'jogador': self.simbolo
        }
    
    def imprimir_estatisticas(self):
        """Imprime estat√≠sticas de forma formatada."""
        stats = self.obter_estatisticas()
        
        print(f"\n{'='*50}")
        print(f"üìä ESTAT√çSTICAS DO AGENTE ({stats.get('jogador', '?')})")
        print(f"{'='*50}")
        print(f"Epis√≥dios treinados:  {stats['episodios']:,}")
        print(f"Estados conhecidos:   {stats['estados_conhecidos']:,}")
        print(f"Epsilon atual:        {stats['epsilon_atual']:.4f}")
        print(f"\n--- Desempenho ---")
        print(f"Vit√≥rias:   {stats.get('vitorias', 0):>6} ({stats['taxa_vitoria']*100:>5.1f}%)")
        print(f"Empates:    {stats.get('empates', 0):>6} ({stats['taxa_empate']*100:>5.1f}%)")
        print(f"Derrotas:   {stats.get('derrotas', 0):>6} ({stats['taxa_derrota']*100:>5.1f}%)")
        print(f"{'='*50}\n")


# ===== FUN√á√ïES AUXILIARES PARA TESTES =====


def testar_inicializacao():
    """Testa a cria√ß√£o de um agente."""
    print("=== TESTE 1: INICIALIZA√á√ÉO ===\n")
    
    agente = AgenteQLearning(
        alpha=0.5,
        gamma=0.9,
        epsilon=0.8,
        jogador=1
    )
    
    print(f"Agente criado: Jogador {agente.simbolo}")
    print(f"Alpha (taxa aprendizado): {agente.alpha}")
    print(f"Gamma (desconto futuro):  {agente.gamma}")
    print(f"Epsilon (explora√ß√£o):     {agente.epsilon}")
    print(f"Q-Table vazia:            {len(agente.q_table)} estados")
    
    agente.imprimir_estatisticas()
    print("=== TESTE CONCLU√çDO ===\n")


def testar_valores_q():
    """Testa obten√ß√£o e atualiza√ß√£o de valores Q."""
    print("=== TESTE 2: VALORES Q ===\n")
    
    agente = AgenteQLearning()
    estado_teste = (1, 0, 0, 0, 2, 0, 0, 0, 0)
    
    print(f"Estado de teste: {estado_teste}")
    print(f"Valor Q inicial (estado, a√ß√£o=4): {agente.obter_valor_q(estado_teste, 4)}")
    
    # Atualiza o valor Q
    agente.atualizar_valor_q(
        estado=estado_teste,
        acao=4,
        recompensa=1.0,
        proximo_estado=(1, 0, 0, 0, 2, 0, 0, 0, 1),
        finalizado=False
    )
    
    print(f"Valor Q ap√≥s atualiza√ß√£o:         {agente.obter_valor_q(estado_teste, 4):.4f}")
    print(f"Tamanho da Q-Table:               {len(agente.q_table)} estado(s)")
    
    print("\n=== TESTE CONCLU√çDO ===\n")


def testar_epsilon_greedy():
    """Testa a estrat√©gia epsilon-greedy."""
    print("=== TESTE 3: EPSILON-GREEDY ===\n")
    
    # Agente com epsilon alto (muita explora√ß√£o)
    agente_explorador = AgenteQLearning(epsilon=0.9)
    
    # Agente com epsilon baixo (muita explora√ß√£o)
    agente_exploitador = AgenteQLearning(epsilon=0.1)
    
    estado = (0, 0, 0, 0, 0, 0, 0, 0, 0)
    acoes = [0, 1, 2, 3, 4, 5, 6, 7, 8]
    
    print("Testando 10 escolhas com EPSILON ALTO (0.9):")
    acoes_explorador = [
        agente_explorador.escolher_acao(estado, acoes, treino=True)
        for _ in range(10)
    ]
    print(f"A√ß√µes escolhidas: {acoes_explorador}")
    print(f"Varia√ß√£o (explora√ß√£o): {len(set(acoes_explorador))} a√ß√µes diferentes")
    
    print("\nTestando 10 escolhas com EPSILON BAIXO (0.1):")
    acoes_exploitador = [
        agente_exploitador.escolher_acao(estado, acoes, treino=True)
        for _ in range(10)
    ]
    print(f"A√ß√µes escolhidas: {acoes_exploitador}")
    print(f"Varia√ß√£o (menos explora√ß√£o): {len(set(acoes_exploitador))} a√ß√µes diferentes")
    
    print("\n=== TESTE CONCLU√çDO ===\n")


def testar_decay_epsilon():
    """Testa o decaimento do epsilon."""
    print("=== TESTE 4: DECAY DO EPSILON ===\n")
    
    agente = AgenteQLearning(
        epsilon=1.0,
        epsilon_min=0.01,
        epsilon_decay=0.95  # Decay r√°pido para visualiza√ß√£o
    )
    
    print(f"Epsilon inicial: {agente.epsilon:.4f}")
    print("\nSimulando 20 epis√≥dios...")
    
    for i in range(20):
        agente.decair_epsilon()
        if i % 5 == 0:
            print(f"Epis√≥dio {i+1:2d}: Epsilon = {agente.epsilon:.4f}")
    
    print(f"\nEpsilon final: {agente.epsilon:.4f}")
    print(f"Epsilon m√≠nimo: {agente.epsilon_min:.4f}")
    print("‚úÖ Epsilon decaiu corretamente sem passar do m√≠nimo!")
    
    print("\n=== TESTE CONCLU√çDO ===\n")


def testar_salvar_carregar():
    """Testa salvar e carregar agente."""
    print("=== TESTE 5: SALVAR E CARREGAR ===\n")
    
    # Cria e treina um agente de exemplo
    print("Criando agente de teste...")
    agente1 = AgenteQLearning(alpha=0.5, gamma=0.9, jogador=1)
    
    # Simula algum "treinamento" (adiciona dados na Q-Table)
    estados_teste = [
        (1, 0, 0, 0, 0, 0, 0, 0, 0),
        (1, 2, 0, 0, 0, 0, 0, 0, 0),
        (1, 2, 1, 0, 0, 0, 0, 0, 0)
    ]
    
    for estado in estados_teste:
        agente1.obter_valor_q(estado, 4)  # Inicializa valores
    
    agente1.episodios_treinados = 1000
    agente1.vitorias = 600
    agente1.derrotas = 200
    agente1.empates = 200
    
    print(f"Q-Table original: {len(agente1.q_table)} estados")
    print(f"Epis√≥dios: {agente1.episodios_treinados}")
    
    # Salva o agente
    print("\nSalvando agente...")
    agente1.salvar("teste_agente.pkl")
    
    # Carrega o agente
    print("\nCarregando agente...")
    agente2 = AgenteQLearning.carregar("teste_agente.pkl")
    
    print(f"\nQ-Table carregada: {len(agente2.q_table)} estados")
    print(f"Epis√≥dios carregados: {agente2.episodios_treinados}")
    
    # Verifica se s√£o iguais
    assert len(agente1.q_table) == len(agente2.q_table), "Q-Tables diferentes!"
    assert agente1.episodios_treinados == agente2.episodios_treinados, "Epis√≥dios diferentes!"
    
    print("\n‚úÖ Salvamento e carregamento funcionaram perfeitamente!")
    
    # Remove arquivo de teste
    Path("teste_agente.pkl").unlink()
    print("(Arquivo de teste removido)")
    
    print("\n=== TESTE CONCLU√çDO ===\n")


def executar_todos_testes():
    """Executa toda a bateria de testes."""
    print("\n" + "="*50)
    print("üß™ BATERIA DE TESTES DO AGENTE Q-LEARNING")
    print("="*50 + "\n")
    
    testar_inicializacao()
    testar_valores_q()
    testar_epsilon_greedy()
    testar_decay_epsilon()
    testar_salvar_carregar()
    
    print("="*50)
    print("‚úÖ TODOS OS TESTES CONCLU√çDOS COM SUCESSO!")
    print("="*50 + "\n")


# Executar testes se rodar este arquivo diretamente
if __name__ == "__main__":
    executar_todos_testes()
