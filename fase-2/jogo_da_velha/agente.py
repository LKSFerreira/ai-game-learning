"""
M√≥dulo: üß† agente.py
Projeto: üìò AI Game Learning

Este m√≥dulo define o Agente que utiliza o algoritmo Q-Learning.
No paradigma de Aprendizado por Refor√ßo, este c√≥digo representa o "Agent".

Responsabilidades do Agente:
- Manter uma "mem√≥ria de jogo", a Tabela Q (Q-Table).
- Decidir qual a√ß√£o tomar, balanceando entre explorar e usar seu conhecimento.
- Aprender com os resultados de suas a√ß√µes, atualizando sua mem√≥ria.
"""

import random
import pickle
from typing import List, Tuple, Dict
from pathlib import Path

class AgenteQLearning:
    """
    Um Agente que aprende a jogar Jogo da Velha usando Q-Learning.
    
    Pense neste Agente como um jogador de Ragnarok Online que est√° aprendendo
    a melhor estrat√©gia para derrotar monstros.

    Hiperpar√¢metros (os "atributos" do nosso jogador):
    - alpha (Œ±): A "Velocidade de Aprendizado".
      * Qu√£o r√°pido o jogador ajusta sua estrat√©gia ap√≥s uma batalha.
      * Valores altos = impulsivo, aprende r√°pido com uma √∫nica experi√™ncia.
      * Valores baixos = c√©tico, precisa de muitas experi√™ncias para mudar de ideia.

    - gamma (Œ≥): A "Vis√£o de Futuro" (Fator de Desconto).
      * O quanto o jogador valoriza recompensas futuras.
      * Valor alto = estrategista, pensa nos pr√≥ximos passos.
      * Valor baixo = imediatista, foca apenas na recompensa de agora.

    - epsilon (Œµ): O "Medidor de Curiosidade" (Taxa de Explora√ß√£o).
      * A chance do jogador tentar uma t√°tica nova e desconhecida.
      * Valor alto = aventureiro, adora explorar o mapa.
      * Valor baixo = conservador, prefere usar a t√°tica que j√° sabe que funciona.
    """

    def __init__(self,
                 alpha: float = 0.5,
                 gamma: float = 0.9,
                 epsilon: float = 1.0,
                 epsilon_minimo: float = 0.01,
                 taxa_decaimento_epsilon: float = 0.9995,
                 jogador: int = 1):
        """
        Inicializa os atributos e a mem√≥ria do Agente.
        """
        # --- HIPERPAR√ÇMETROS (Atributos do Agente) ---
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_minimo = epsilon_minimo
        self.taxa_decaimento_epsilon = taxa_decaimento_epsilon

        # --- IDENTIDADE ---
        self.jogador = jogador
        self.simbolo = 'X' if jogador == 1 else 'O'

        # --- MEM√ìRIA (A "Enciclop√©dia de Monstros" do Jogador) ---
        self.tabela_q: Dict[Tuple, Dict[int, float]] = {}

        # --- ESTAT√çSTICAS DE TREINO ---
        self.partidas_treinadas = 0
        self.vitorias = 0
        self.derrotas = 0
        self.empates = 0
        
        # --- MEM√ìRIA DE CURTO PRAZO (para a partida atual) ---
        self.historico_partida: List[Tuple[Tuple, int]] = []

    def obter_valor_q(self, estado: Tuple, acao: int) -> float:
        """
        Consulta a "mem√≥ria" para ver o valor de uma a√ß√£o em um estado.
        Se o Agente nunca viu essa situa√ß√£o antes, ele assume que o valor √© 0.
        """
        if estado not in self.tabela_q:
            self.tabela_q[estado] = {}
        if acao not in self.tabela_q[estado]:
            self.tabela_q[estado][acao] = 0.0
        return self.tabela_q[estado][acao]

    def atualizar_valor_q(self, estado: Tuple, acao: int, recompensa: float, proximo_estado: Tuple):
        """
        Atualiza a "mem√≥ria" do Agente usando a Equa√ß√£o de Bellman.
        √â aqui que o aprendizado realmente acontece.
        """
        opiniao_antiga = self.obter_valor_q(estado, acao)
        melhor_valor_futuro = self._obter_melhor_valor_q_do_estado(proximo_estado)
        valor_real_da_jogada = recompensa + self.gamma * melhor_valor_futuro
        surpresa = valor_real_da_jogada - opiniao_antiga
        novo_valor_q = opiniao_antiga + self.alpha * surpresa
        self.tabela_q[estado][acao] = novo_valor_q

    def _obter_melhor_valor_q_do_estado(self, estado: Tuple) -> float:
        """
        Verifica na "mem√≥ria" qual √© a melhor jogada poss√≠vel a partir de um estado.
        """
        if estado not in self.tabela_q or not self.tabela_q[estado]:
            return 0.0
        return max(self.tabela_q[estado].values())

    def escolher_acao(self, estado: Tuple, acoes_validas: List[int], em_treinamento: bool = True) -> int:
        """
        Decide qual jogada fazer usando a estrat√©gia Epsilon-Greedy.
        """
        if not acoes_validas:
            raise ValueError("N√£o h√° a√ß√µes v√°lidas para escolher.")
        if not em_treinamento:
            return self._escolher_melhor_acao(estado, acoes_validas)
        if random.random() < self.epsilon:
            return random.choice(acoes_validas)
        else:
            return self._escolher_melhor_acao(estado, acoes_validas)

    def _escolher_melhor_acao(self, estado: Tuple, acoes_validas: List[int]) -> int:
        """
        Consulta a "mem√≥ria" e escolhe a a√ß√£o com o maior valor Q.
        """
        valores_q_das_acoes = {acao: self.obter_valor_q(estado, acao) for acao in acoes_validas}
        valor_maximo_q = max(valores_q_das_acoes.values())
        melhores_acoes = [acao for acao, valor in valores_q_das_acoes.items() if valor == valor_maximo_q]
        return random.choice(melhores_acoes)

    # --- M√âTODOS PARA O CICLO DE TREINAMENTO (GERENCIADOS PELO TREINADOR) ---

    def iniciar_nova_partida(self):
        """ Limpa a mem√≥ria de curto prazo para o in√≠cio de uma nova partida. """
        self.historico_partida = []

    def registrar_jogada(self, estado: Tuple, acao: int):
        """ Guarda a jogada (estado, a√ß√£o) feita nesta partida. """
        self.historico_partida.append((estado, acao))

    def aprender_com_fim_de_partida(self, recompensa_final: float):
        """
        Processa o hist√≥rico da partida finalizada para aprender com ela.
        Este m√©todo √© chamado pelo Treinador ao final de cada jogo.
        """
        self.partidas_treinadas += 1
        if recompensa_final > 0: self.vitorias += 1
        elif recompensa_final < 0: self.derrotas += 1
        else: self.empates += 1

        # Propaga a recompensa final para tr√°s, valorizando as jogadas
        # que levaram a este resultado.
        for estado, acao in reversed(self.historico_partida):
            # Para este m√©todo de aprendizado, o "pr√≥ximo estado" n√£o √© relevante,
            # apenas a recompensa final que foi alcan√ßada.
            self.atualizar_valor_q(estado, acao, recompensa_final, estado)
            # A recompensa perde um pouco de for√ßa a cada passo para tr√°s.
            recompensa_final *= self.gamma
        
        self.reduzir_epsilon()

    def reduzir_epsilon(self):
        """
        Reduz a "curiosidade" do Agente, tornando-o mais confiante em seu
        conhecimento com o passar do tempo.
        """
        self.epsilon = max(self.epsilon_minimo, self.epsilon * self.taxa_decaimento_epsilon)

    def salvar_memoria(self, caminho: str = "agente_treinado.pkl"):
        """
        Salva o conhecimento do Agente (a Tabela Q) em um arquivo.
        """
        caminho_arquivo = Path(caminho)
        caminho_arquivo.parent.mkdir(parents=True, exist_ok=True)
        
        with open(caminho_arquivo, 'wb') as arquivo:
            pickle.dump(self.tabela_q, arquivo)
        print(f"üíæ Mem√≥ria do Agente ({self.simbolo}) salva em: {caminho_arquivo}")

    @classmethod
    def carregar(cls, caminho: str, **kwargs) -> 'AgenteQLearning':
        """
        Cria uma inst√¢ncia de Agente e carrega seu conhecimento de um arquivo.
        Permite sobrescrever hiperpar√¢metros no momento do carregamento.
        """
        # Cria um novo agente, passando quaisquer hiperpar√¢metros customizados
        agente = cls(**kwargs)
        
        caminho_arquivo = Path(caminho)
        if caminho_arquivo.exists():
            with open(caminho_arquivo, 'rb') as arquivo:
                agente.tabela_q = pickle.load(arquivo)
            print(f"‚úÖ Mem√≥ria do Agente ({agente.simbolo}) carregada de: {caminho_arquivo}")
            print(f"   - O Agente conhece {len(agente.tabela_q):,} situa√ß√µes de jogo.")
        else:
            print(f"‚ö†Ô∏è  Aviso: Nenhum arquivo de mem√≥ria encontrado em {caminho}. O Agente ({agente.simbolo}) come√ßar√° do zero.")
        return agente

    def obter_estatisticas(self) -> Dict:
        """ Retorna um dicion√°rio com as estat√≠sticas de desempenho do Agente. """
        total_jogos = self.vitorias + self.derrotas + self.empates
        if total_jogos == 0: return {"taxa_vitoria": 0.0, "taxa_empate": 0.0, "taxa_derrota": 0.0}

        return {
            'partidas_treinadas': self.partidas_treinadas,
            'estados_conhecidos': len(self.tabela_q),
            'vitorias': self.vitorias,
            'derrotas': self.derrotas,
            'empates': self.empates,
            'taxa_vitoria': self.vitorias / total_jogos,
            'taxa_empate': self.empates / total_jogos,
            'taxa_derrota': self.derrotas / total_jogos,
            'epsilon_atual': self.epsilon,
            'jogador': self.simbolo
        }

    def imprimir_estatisticas(self):
        """ Imprime as estat√≠sticas de forma leg√≠vel no console. """
        stats = self.obter_estatisticas()
        
        print(f"\n{'='*50}")
        print(f"üìä ESTAT√çSTICAS DO AGENTE ({stats.get('jogador', '?')})")
        print(f"{'='*50}")
        print(f"Partidas treinadas:   {stats.get('partidas_treinadas', 0):,}")
        print(f"Estados conhecidos:   {stats.get('estados_conhecidos', 0):,}")
        print(f"Curiosidade (Epsilon):{stats.get('epsilon_atual', 0.0):.4f}")
        print(f"\n--- Desempenho ---")
        print(f"Vit√≥rias:   {stats.get('vitorias', 0):>6} ({stats.get('taxa_vitoria', 0.0)*100:>5.1f}%)")
        print(f"Empates:    {stats.get('empates', 0):>6} ({stats.get('taxa_empate', 0.0)*100:>5.1f}%)")
        print(f"Derrotas:   {stats.get('derrotas', 0):>6} ({stats.get('taxa_derrota', 0.0)*100:>5.1f}%)")
        print(f"{'='*50}\n")