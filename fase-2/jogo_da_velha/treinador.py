"""
M√≥dulo: üèõÔ∏è treinador.py
Projeto: üìò AI Game Learning

Este m√≥dulo √© o "Mestre da Guilda" ou o "Dungeon Master".
Ele √© respons√°vel por orquestrar o treinamento dos Agentes,
colocando-os para batalhar entre si em um processo chamado "self-play".

Responsabilidades:
- Gerenciar o loop de treinamento principal (milhares de partidas).
- Coordenar a intera√ß√£o entre os Agentes e o Ambiente.
- Atribuir as recompensas corretas a cada Agente no final da partida.
- Exibir estat√≠sticas de treinamento em tempo real com uma interface rica.
- Salvar o conhecimento (modelos) e estat√≠sticas dos Agentes treinados.
"""

from datetime import datetime
import os
from pathlib import Path
from typing import Tuple

# Tenta importar a biblioteca 'rich' para uma interface visual avan√ßada.
# Se n√£o estiver instalada, define RICH_DISPONIVEL como False.
try:
    from rich.live import Live
    from rich.table import Table
    from rich.panel import Panel
    from rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn
    RICH_DISPONIVEL = True
except ImportError:
    RICH_DISPONIVEL = False

# Tqdm √© usado como uma alternativa mais simples se 'rich' n√£o estiver dispon√≠vel.
from tqdm import tqdm

from ambiente import AmbienteJogoDaVelha
from agente import AgenteQLearning

class Treinador:
    """
    Orquestra o treinamento de dois agentes Q-Learning atrav√©s de self-play,
    com uma interface de usu√°rio rica para acompanhamento em tempo real.
    """
    def __init__(self, agente_x: AgenteQLearning, agente_o: AgenteQLearning, ambiente: AmbienteJogoDaVelha):
        self.agente_x = agente_x
        self.agente_o = agente_o
        self.ambiente = ambiente
        self.pasta_modelos = Path("modelos_treinados")
        self.pasta_modelos.mkdir(exist_ok=True)
        self._checkpoints = []  # Lista para armazenar metadados dos checkpoints


    def executar_uma_partida(self) -> int:
        """ Executa uma √∫nica partida (um epis√≥dio) entre os dois agentes. """
        self.ambiente.reiniciar_partida()
        self.agente_x.iniciar_nova_partida()
        self.agente_o.iniciar_nova_partida()

        while not self.ambiente.partida_finalizada:
            agente_da_vez = self.agente_x if self.ambiente.jogador_atual == 1 else self.agente_o
            estado_atual = self.ambiente.obter_estado_como_tupla()
            acoes_validas = self.ambiente.obter_acoes_validas()
            acao_escolhida = agente_da_vez.escolher_acao(estado_atual, acoes_validas, em_treinamento=True)
            agente_da_vez.registrar_jogada(estado_atual, acao_escolhida)
            self.ambiente.executar_jogada(acao_escolhida)

        if self.ambiente.vencedor == 1:
            recompensa_x, recompensa_o = 1.0, -1.0
        elif self.ambiente.vencedor == 2:
            recompensa_x, recompensa_o = -1.0, 1.0
        else:
            recompensa_x, recompensa_o = 0.0, 0.0
            
        self.agente_x.aprender_com_fim_de_partida(recompensa_x)
        self.agente_o.aprender_com_fim_de_partida(recompensa_o)
        
        return self.ambiente.vencedor

    def treinar(self, numero_de_partidas: int = 50000, intervalo_log: int = 1000, intervalo_checkpoint: int = 10000):
        """ Executa o loop de treinamento principal com interface visual. """
        print("\n" + "="*50)
        print("‚öîÔ∏è INICIANDO TREINAMENTO INTENSIVO (SELF-PLAY) ‚öîÔ∏è")
        print("="*50)
        print(f"Total de Partidas: {numero_de_partidas:,}")
        print(f"Interface Gr√°fica: {'Rich (Avan√ßada)' if RICH_DISPONIVEL else 'TQDM (B√°sica)'}")
        print("="*50 + "\n")

        # Inicializa lista de checkpoints
        self._checkpoints = []
        vitorias_x_janela, vitorias_o_janela, empates_janela = 0, 0, 0
        ultimo_checkpoint = None

        if RICH_DISPONIVEL:
            # --- MODO RICH (Interface Avan√ßada) ---
            progresso = Progress(
                TextColumn("[bold blue]{task.description}"), 
                BarColumn(), 
                TextColumn("{task.percentage:>3.0f}%"), 
                TimeRemainingColumn()
            )
            id_tarefa = progresso.add_task("Treinando", total=numero_de_partidas)

            def gerar_painel_estatisticas() -> Panel:
                tabela = Table.grid(expand=True)
                tabela.add_column(justify="left")
                tabela.add_column(justify="right")
                total_janela = vitorias_x_janela + vitorias_o_janela + empates_janela or 1
                tabela.add_row("Vit√≥rias X (janela)", f"[bold green]{vitorias_x_janela}[/]")
                tabela.add_row("Vit√≥rias O (janela)", f"[bold yellow]{vitorias_o_janela}[/]")
                tabela.add_row("Empates (janela)", f"{empates_janela}")
                tabela.add_row("Taxa de Empate %", f"{(empates_janela / total_janela) * 100:.1f}%")
                tabela.add_row("-" * 20, "-" * 20)
                tabela.add_row("Epsilon X", f"{self.agente_x.epsilon:.6f}")
                tabela.add_row("Epsilon O", f"{self.agente_o.epsilon:.6f}")
                tabela.add_row("Estados Conhecidos X", f"{len(self.agente_x.tabela_q):,}")
                tabela.add_row("Estados Conhecidos O", f"{len(self.agente_o.tabela_q):,}")
                tabela.add_row("√öltimo Checkpoint", f"{ultimo_checkpoint or 'Nenhum'}")
                return Panel(tabela, title="[bold]Estat√≠sticas da Janela[/]", border_style="blue")

            def gerar_layout():
                """Gera o layout completo com progresso e estat√≠sticas"""
                layout = Table.grid(expand=True)
                layout.add_row(
                    Panel(progresso, title="[bold]Progresso Geral[/]", border_style="green"), 
                    gerar_painel_estatisticas()
                )
                return layout

            with Live(gerar_layout(), refresh_per_second=10) as live:
                for i in range(numero_de_partidas):
                    vencedor = self.executar_uma_partida()
                    if vencedor == 1: 
                        vitorias_x_janela += 1
                    elif vencedor == 2: 
                        vitorias_o_janela += 1
                    else: 
                        empates_janela += 1

                    progresso.update(id_tarefa, advance=1)

                    if (i + 1) % intervalo_log == 0:
                        vitorias_x_janela, vitorias_o_janela, empates_janela = 0, 0, 0
                    
                    if (i + 1) % intervalo_checkpoint == 0:
                        self._salvar_checkpoint(i + 1)
                        ultimo_checkpoint = f"{i+1:,}"

                    # Atualiza o layout completo periodicamente
                    if i % 250 == 0:
                        live.update(gerar_layout())
                
                # Atualiza√ß√£o final
                live.update(gerar_layout())
        else:
            # --- MODO TQDM (Interface B√°sica) ---
            for i in tqdm(range(numero_de_partidas), desc="Treinando"):
                vencedor = self.executar_uma_partida()
                if vencedor == 1: 
                    vitorias_x_janela += 1
                elif vencedor == 2: 
                    vitorias_o_janela += 1
                else: 
                    empates_janela += 1

                if (i + 1) % intervalo_checkpoint == 0:
                    self._salvar_checkpoint(i + 1)
        
        # Exibe resumo de checkpoints
        self._exibir_resumo_checkpoints()
        
        print("\n" + "="*50)
        print("‚úÖ TREINAMENTO CONCLU√çDO!")
        print("="*50)
        
        self.agente_x.imprimir_estatisticas()
        self.agente_o.imprimir_estatisticas()
        
        self._salvar_modelos_finais()

    def _salvar_checkpoint(self, numero_partida: int):
        """ Salva o estado atual dos agentes em um checkpoint. """
        caminho_x = self.pasta_modelos / f"agente_x_checkpoint_{numero_partida}.pkl"
        caminho_o = self.pasta_modelos / f"agente_o_checkpoint_{numero_partida}.pkl"
        self.agente_x.salvar_memoria(str(caminho_x))
        self.agente_o.salvar_memoria(str(caminho_o))

    def _salvar_modelos_finais(self):
        """ Salva os modelos finais ap√≥s o t√©rmino do treinamento. """
        caminho_x = self.pasta_modelos / f"agente_x_final_{self.ambiente.dimensao}x{self.ambiente.dimensao}.pkl"
        caminho_o = self.pasta_modelos / f"agente_o_final_{self.ambiente.dimensao}x{self.ambiente.dimensao}.pkl"
        self.agente_x.salvar_memoria(str(caminho_x))
        self.agente_o.salvar_memoria(str(caminho_o))
    
    def _salvar_checkpoint(self, numero_partida: int):
        """ Salva o estado atual dos agentes em um checkpoint e registra metadados. """
        caminho_x = self.pasta_modelos / f"agente_x_checkpoint_{numero_partida}.pkl"
        caminho_o = self.pasta_modelos / f"agente_o_checkpoint_{numero_partida}.pkl"
        
        try:
            self.agente_x.salvar_memoria(str(caminho_x))
            self.agente_o.salvar_memoria(str(caminho_o))
            
            # Registra metadados do checkpoint
            self._checkpoints.append({
                'numero_partida': numero_partida,
                'timestamp': datetime.now(),
                'pasta': str(self.pasta_modelos),
                'sucesso': True
            })
        except Exception as e:
            # Registra falha
            self._checkpoints.append({
                'numero_partida': numero_partida,
                'timestamp': datetime.now(),
                'erro': str(e),
                'sucesso': False
            })

    def _exibir_resumo_checkpoints(self):
        """ Exibe um resumo limpo e organizado dos checkpoints salvos. """
        if not self._checkpoints or len(self._checkpoints) == 0:
            print('\n‚ö†Ô∏è  Nenhum checkpoint foi salvo.\n')
            return

        checkpoints_sucesso = [cp for cp in self._checkpoints if cp.get('sucesso', False)]
        
        print('\n' + '‚îÅ' * 80)
        print('üíæ CHECKPOINTS SALVOS')
        print('‚îÅ' * 80 + '\n')
        
        if checkpoints_sucesso:
            print(f'‚úÖ {len(checkpoints_sucesso)} checkpoint(s) criado(s) com sucesso\n')
            print(f'üìÅ Pasta: {checkpoints_sucesso[0]["pasta"]}\n')
            
            for cp in checkpoints_sucesso:
                data_formatada = cp['timestamp'].strftime('%d/%m/%Y √†s %H:%M')
                partida_formatada = f"{cp['numero_partida']:,}".replace(',', '.')
                print(f'  üéØ Partida {partida_formatada} ‚Äî {data_formatada}')
        
        # Exibe erros se houver
        checkpoints_erro = [cp for cp in self._checkpoints if not cp.get('sucesso', False)]
        if checkpoints_erro:
            print(f'\n‚ùå {len(checkpoints_erro)} checkpoint(s) com erro:\n')
            for cp in checkpoints_erro:
                partida_formatada = f"{cp['numero_partida']:,}".replace(',', '.')
                print(f'  ‚ö†Ô∏è  Partida {partida_formatada} ‚Äî {cp.get("erro", "Erro desconhecido")}')
        
        print('\n' + '‚îÅ' * 80 + '\n')

    
    def avaliar_agentes(self, numero_de_partidas: int = 10000):
            """
            Coloca os agentes para jogar um contra o outro em modo de "performance m√°xima",
            com uma interface rica para acompanhamento em tempo real.
            """
            print("\n" + "="*50)
            print("üèÜ INICIANDO MODO DE AVALIA√á√ÉO (SEM EXPLORA√á√ÉO) üèÜ")
            print("="*50)

            # --- L√ìGICA DE CARREGAMENTO AUTOM√ÅTICO ---
            if not self.agente_x.tabela_q:
                print("Agente X n√£o treinado. Tentando carregar modelo do disco...")
                caminho_x = self.pasta_modelos / f"superagente_final_{self.ambiente.dimensao}x{self.ambiente.dimensao}.pkl"
                self.agente_x = AgenteQLearning.carregar(str(caminho_x), jogador=1)

            if not self.agente_o.tabela_q:
                print("Agente O n√£o treinado. Tentando carregar modelo do disco...")
                caminho_o = self.pasta_modelos / f"agente_o_final_{self.ambiente.dimensao}x{self.ambiente.dimensao}.pkl"
                self.agente_o = AgenteQLearning.carregar(str(caminho_o), jogador=2)
            
            vitorias_x, vitorias_o, empates = 0, 0, 0

            if RICH_DISPONIVEL:
                progresso = Progress(
                    TextColumn("[bold blue]{task.description}"), 
                    BarColumn(), 
                    TextColumn("{task.percentage:>3.0f}%"), 
                    TimeRemainingColumn()
                )
                id_tarefa = progresso.add_task("Avaliando", total=numero_de_partidas)

                def gerar_painel_estatisticas_avaliacao() -> Panel:
                    tabela = Table.grid(expand=True)
                    tabela.add_column(justify="left")
                    tabela.add_column(justify="right")
                    total_partidas = vitorias_x + vitorias_o + empates or 1
                    
                    tabela.add_row("Vit√≥rias X", f"[bold green]{vitorias_x}[/]")
                    tabela.add_row("Vit√≥rias O", f"[bold yellow]{vitorias_o}[/]")
                    tabela.add_row("Empates", f"{empates}")
                    tabela.add_row("-" * 20, "-" * 20)
                    tabela.add_row("Taxa de Empate %", f"{(empates / total_partidas) * 100:.2f}%")
                    tabela.add_row("Estados Conhecidos X", f"{len(self.agente_x.tabela_q):,}")
                    tabela.add_row("Estados Conhecidos O", f"{len(self.agente_o.tabela_q):,}")
                    
                    return Panel(tabela, title="[bold]Estat√≠sticas em Tempo Real[/]", border_style="blue")

                def gerar_layout():
                    """Gera o layout completo"""
                    layout = Table.grid(expand=True)
                    layout.add_row(
                        Panel(progresso, title="[bold]Progresso da Avalia√ß√£o[/]", border_style="green"), 
                        gerar_painel_estatisticas_avaliacao()
                    )
                    return layout

                with Live(gerar_layout(), refresh_per_second=10) as live:
                    for i in range(numero_de_partidas):
                        self.ambiente.reiniciar_partida()
                        while not self.ambiente.partida_finalizada:
                            agente_da_vez = self.agente_x if self.ambiente.jogador_atual == 1 else self.agente_o
                            estado = self.ambiente.obter_estado_como_tupla()
                            acoes = self.ambiente.obter_acoes_validas()
                            acao = agente_da_vez.escolher_acao(estado, acoes, em_treinamento=False)
                            self.ambiente.executar_jogada(acao)

                        if self.ambiente.vencedor == 1: 
                            vitorias_x += 1
                        elif self.ambiente.vencedor == 2: 
                            vitorias_o += 1
                        else: 
                            empates += 1
                        
                        progresso.update(id_tarefa, advance=1)
                        
                        # Atualiza o layout periodicamente
                        if i % 250 == 0:
                            live.update(gerar_layout())
                    
                    # Atualiza√ß√£o final garantida
                    live.update(gerar_layout())

            else:
                # Fallback para TQDM
                for _ in tqdm(range(numero_de_partidas), desc="Avaliando Performance"):
                    # ... (l√≥gica do TQDM permanece a mesma) ...
                    self.ambiente.reiniciar_partida()
                    while not self.ambiente.partida_finalizada:
                        agente_da_vez = self.agente_x if self.ambiente.jogador_atual == 1 else self.agente_o
                        estado = self.ambiente.obter_estado_como_tupla()
                        acoes = self.ambiente.obter_acoes_validas()
                        acao = agente_da_vez.escolher_acao(estado, acoes, em_treinamento=False)
                        self.ambiente.executar_jogada(acao)

                    if self.ambiente.vencedor == 1: vitorias_x += 1
                    elif self.ambiente.vencedor == 2: vitorias_o += 1
                    else: empates += 1

            print("\n--- RESULTADO FINAL DA AVALIA√á√ÉO ---")
            print(f"Partidas Jogadas: {numero_de_partidas}")
            print(f"Vit√≥rias de X: {vitorias_x} ({(vitorias_x/numero_de_partidas)*100:.1f}%)")
            print(f"Vit√≥rias de O: {vitorias_o} ({(vitorias_o/numero_de_partidas)*100:.1f}%)")
            print(f"Empates: {empates} ({(empates/numero_de_partidas)*100:.1f}%)")
            print("="*50 + "\n")
    
    def mesclar_agentes_treinados(self):
        """
        Executa o script mesclar_modelos.py para criar o superagente.
        """
        print("\n" + "="*50)
        print("üîÑ EXECUTANDO MESCLAGEM DOS MODELOS...")
        print("="*50 + "\n")
        
        # Executa o script Python
        codigo_retorno = os.system('py -m mesclar_modelos')
        
        if codigo_retorno == 0:
            print("\n‚úÖ Mesclagem conclu√≠da com sucesso!")
        else:
            print(f"\n‚ùå Erro ao executar mesclagem (c√≥digo: {codigo_retorno})")

# --- Bloco de Execu√ß√£o Principal ---
if __name__ == "__main__":
    # Op√ß√£o 1: Treinamento Padr√£o (3x3, 50.000 partidas)
    # Roda um treinamento completo e salva os modelos.
    ambiente_padrao = AmbienteJogoDaVelha(dimensao=3)
    agente_x_padrao = AgenteQLearning(jogador=1)
    agente_o_padrao = AgenteQLearning( jogador=2)
    
    treinador_padrao = Treinador(agente_x_padrao, agente_o_padrao, ambiente_padrao)
    treinador_padrao.treinar(numero_de_partidas=200000, intervalo_log=500, intervalo_checkpoint=40000)
    
    treinador_padrao.avaliar_agentes()
    
    treinador_padrao.mesclar_agentes_treinados()
    
    # Op√ß√£o 2: Treinamento Customizado (ex: 4x4, 100.000 partidas)
    # Descomente as linhas abaixo para rodar um treino diferente.
    # print("\n--- INICIANDO TREINAMENTO CUSTOMIZADO 4x4 ---")
    # ambiente_4x4 = AmbienteJogoDaVelha(dimensao=4)
    # agente_x_4x4 = AgenteQLearning(jogador=1, taxa_decaimento_epsilon=0.9999)
    # agente_o_4x4 = AgenteQLearning(jogador=2, taxa_decaimento_epsilon=0.9999)
    #
    # treinador_4x4 = Treinador(agente_x_4x4, agente_o_4x4, ambiente_4x4)
    # treinador_4x4.treinar(numero_de_partidas=100000, intervalo_log=10000)